{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Méthodes non paramétriques\n",
    "\n",
    "\n",
    "L'objectif de cet exercice est de mettre en pratique les notions d'estimation de densité de probabilité et de classification utilisant des modèles non paramétriques.\n",
    "\n",
    "Soit ${\\bf x}_0$ un vecteur de caractéristiques et ${\\cal D}({\\bf x}_0)$ un domaine comprenant ce vecteur ${\\bf x}_0$, nous voulons construire un estimateur $\\hat{P}({\\bf x}_0|c)$ :\n",
    "$$\n",
    "\\hat{P}({\\bf x}_0|c) \\approx \\cfrac{\\frac{k}{n}}{V({\\cal D}({\\bf x}_0))}\n",
    "$$\n",
    "$k$ est le nombre d'échantillons qui appartiennent au domaine ${\\cal D}$ et $n$ est le nombre total d'échantillons.\n",
    "Comme nous l'avons vu en classe, il existe deux principales catégories d'approches de modélisation non paramétriques :\n",
    "+  lier $V({\\cal D}_n({\\bf x}_0))$ à n : il s'agit des modèles basés sur des noyaux (Kernel Density Estimation également appelés fenêtres de Parzen)\n",
    "+ lier $k_n$ en fonction de $n$ et ajuster le domaine  $V({\\cal D}_n({\\bf x}_0))$ de manière à ce que $k$ échantillons appartiennent à ${\\cal D}_n({\\bf x}_0)$ : c'est la méthode des $k$ plus proches voisins (KNN)\n",
    "\n",
    "\n",
    "\n",
    "# 2. Estimation par KNN\n",
    "\n",
    "\n",
    "## 2.a)  Estimation de distribution par KNN\n",
    "\n",
    "\n",
    "L'estimateur de densité de probabilité par KNN est calculé par la fonction suivante : \n",
    "$$\n",
    "\\hat{P}({\\bf x}_0|\\class)=\\cfrac{\\cfrac{k_n}{n}}{V[{\\cal D}({\\bf x}_0,\\alpha)]}\n",
    "$$\n",
    "avec $k_n$ : nombre d'échantillons dans le domaine ${\\cal D}$, $n$ nombre total d'échantillons et $\\alpha$ un facteur d'échelle. L'idée principale du modèle KNN est d'ajuster $\\alpha$ de manière à inclure $k_n$ échantillons\n",
    "dans le domaine ${\\cal D}$.\n",
    "\n",
    "\n",
    "### A Faire\n",
    "+ Complétez le code ci-dessous qui charge les datasets, calcule les caractéristiques, applique la méthode KNN et affiche la densité. L'option `n_neighbors=x` définit le nombre de voisins $k_n$. Utilisez $k_n=1$,\n",
    "+ Changez $k_n$ pour 2, 4 puis 6 et comparez la fonction de densité de probabilité correspondante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des bibliothèques python nécessaires :\n",
    "import numpy as np\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# Pour avoir un plot 3D interactif\n",
    "%matplotlib widget\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Gaussienne et KNN\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "\n",
    "# pour manipuler les accès aux répertoires\n",
    "import os\n",
    "\n",
    "# Pour le traitement d'images et la vision par ordinateur\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix du dataset\n",
    "training_images_path ='images/dataset1/app/'\n",
    "testing_images_path ='images/dataset1/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'extraction de caractéristiques d'une image\n",
    "def getfeature(filename):\n",
    "    im = cv2.imread(filename)\n",
    "    imrgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    R = imrgb[:,:,0]\n",
    "    G = imrgb[:,:,1]\n",
    "    B = imrgb[:,:,2]    \n",
    "    In= R+G+B # Calcul de l'intensité des pixels\n",
    "    mIn = np.max((In.flatten())) # Calcul de l'intensité maximale\n",
    "    # normalisation\n",
    "    Rn = R/mIn\n",
    "    Gn = G/mIn\n",
    "    Bn = B/mIn    \n",
    "    # Calcul de la moyenne de chaque composante\n",
    "    X = [np.mean(Rn.flatten()),np.mean(Gn.flatten()),np.mean(Bn.flatten())]    \n",
    "    return X\n",
    "\n",
    "# Fonction d'extraction de caractéristiques pour toutes les images d'un répertoire de la classe cl\n",
    "def getfeatures (path, cl):\n",
    "    X = []\n",
    "    scl = 'c'+str(cl)\n",
    "    for i in os.listdir(path):\n",
    "        #print(\"processing file \",i)\n",
    "        if os.path.isfile(os.path.join(path,i)) and scl in i:\n",
    "            X.append(getfeature(path+i))\n",
    "    print(\"Calcul des caractéristiques de la classe\", cl, \"terminé\")\n",
    "    return np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupérer les caractéristiques pour les **2 classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETER\n",
    "\n",
    "# Calcul des caractéristiques pour les images de la base d'entrainement :\n",
    "X1 = \n",
    "X2 = \n",
    "\n",
    "# Calcul des caractéristiques pour les images de la base de test :\n",
    "Xtest1 = \n",
    "Xtest2 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des données dans l'espace des caractéristiques\n",
    "plt.figure()\n",
    "plt.plot(X1[:,0],X1[:,1],'+',label='Classe 1')\n",
    "plt.plot(X2[:,0],X2[:,1],'o',label = 'Classe 2')\n",
    "plt.legend()\n",
    "plt.title('Dataset d\\'entrainement dans l\\'espace de caractéristiques')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquez la méthode des k plus proches voisins ici\n",
    "\n",
    "Vous pourrez vous référer à la documentation :\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html?highlight=nearestneighbors#sklearn.neighbors.NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choix de k (nombre de voisins)\n",
    "k = 1\n",
    "\n",
    "# calcul des distributions de k plus proches voisins\n",
    "knn1 = \n",
    "knn2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les voisins du point de coordonnées (0.2,0.7)\n",
    "n = knn1.kneighbors([[0.2 , 0.7]])\n",
    "print(n)\n",
    "print(X1[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage pdf estimation \n",
    "def plotpdf(pdf, Xdata, x_min=-1, x_max=1, y_min=-1, y_max=1, res=300, title = 'KNN pdf'):\n",
    "    #Create grid and multivariate KNN estimation\n",
    "    x = np.linspace(x_min,x_max,res)\n",
    "    y = np.linspace(y_min,y_max,res)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    pos = np.empty(X.shape + (2,))\n",
    "    pos[:, :, 0] = X; pos[:, :, 1] = Y\n",
    "    \n",
    "    # Distances\n",
    "    distances, indices = pdf.kneighbors(np.transpose(np.array([X.flatten(),Y.flatten()])))\n",
    "    distances = np.mean(distances,axis=1)\n",
    "    distances = distances.reshape(X.shape[0],X.shape[1])\n",
    "\n",
    "    #Make a 3D plot    \n",
    "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}) \n",
    "    ax.plot_surface(X, Y, 1 / distances ,cmap='viridis',linewidth=0)\n",
    "    ax.scatter(Xdata[:,0],Xdata[:,1])\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('Z axis')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "plotpdf(knn1, X1,x_min=0, y_min=0, title='KNN pdf pour la classe 1')\n",
    "plt.plot\n",
    "plotpdf(knn2, X1,x_min=0, y_min=0, title='KNN pdf pour la classe 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b)  Classification par KNN\n",
    "\n",
    "KNN est très utilisé pour des problèmes de classification. La distribution à postériori, à partir de la règle de Bayes peut être écrite à partir des échantillons de la manière suivante :\n",
    "$$\n",
    "p(c_i|{\\bf x})=\\cfrac{{p({\\bf x}|c_i)}. P(c_i)}{\\sum_{j=1}^m \n",
    "\tp({\\bf x}|c_j)P(c_j)}\n",
    "\\approx\\cfrac{\\cfrac{\\frac{k_i}{n}}{V}}\n",
    "{\\sum_{j=1}^m\\cfrac{\\frac{k_i}{n}}{V}}\n",
    "$$\n",
    "et se simplifie par :\n",
    "$$\n",
    "p(c_i|{\\bf x})\\approx\\cfrac{k_i}{k}\n",
    "$$\n",
    "\n",
    "\n",
    "+ Exécutez le code qui affiche la densité à postériori de la classe 1.\n",
    "+ Changez le nombre de voisins et comparez les distributions à postériori résultantes.\n",
    "+ Modifiez le code pour calculer l'erreur de classification sur la base de test. Vous pourrez utilisez la fonction `KneighborsClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotKNNdistribution(knn1,knn2, Xdata1, Xdata2,x_min=-1, x_max=1, y_min=-1, y_max=1, res=300\\\n",
    "             , title = 'KNN fonction de décision pour la classe 1'):\n",
    "    #Create grid and multivariate KDE estimation\n",
    "    x = np.linspace(x_min,x_max,res)\n",
    "    y = np.linspace(y_min,y_max,res)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    pos = np.empty(X.shape + (2,))\n",
    "    pos[:, :, 0] = X; pos[:, :, 1] = Y\n",
    "    \n",
    "    distances1, indices = knn1.kneighbors(np.transpose(np.array([X.flatten(),Y.flatten()])))\n",
    "    distances1 = np.mean(distances1,axis=1)\n",
    "    distances1 = distances1.reshape(X.shape[0],X.shape[1])\n",
    "    distances2, indices = knn2.kneighbors(np.transpose(np.array([X.flatten(),Y.flatten()])))\n",
    "    distances2 = np.mean(distances2,axis=1)\n",
    "    distances2 = distances2.reshape(X.shape[0],X.shape[1])\n",
    " \n",
    "    #Make a 3D plot\n",
    "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "    ax.plot_surface(X, Y, \\\n",
    "        (1/distances1)/(1/distances1+1/distances2),\\\n",
    "        cmap='viridis',linewidth=0,vmin=0,vmax=1, alpha = 0.6)\n",
    "    \n",
    "    ax.plot_surface(X, Y, \\\n",
    "        (1/distances2)/(1/distances1+1/distances2),\\\n",
    "        cmap='Oranges',linewidth=0,vmin=0,vmax=1, alpha = 0.6)\n",
    "    \n",
    "    ax.scatter(Xdata1[:,0],Xdata1[:,1],label='Classe 1')\n",
    "    ax.scatter(Xdata2[:,0],Xdata2[:,1],label='Classe 2')\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('Z axis')\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "\n",
    "plotKNNdistribution(pdf1,pdf2,X1,X2,x_min=0, y_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNearestNeighbors Classifier\n",
    "#Compléter\n",
    "clf = KNeighborsClassifier(k)\n",
    "\n",
    "clf.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot KDE based decision function \n",
    "def plotKNNdecision(knn1,knn2, Xdata1, Xdata2,x_min=-1, x_max=1, y_min=-1, y_max=1, res=300, title = 'Frontière de décision'):\n",
    "    #Create grid and multivariate KDE estimation\n",
    "    x = np.linspace(x_min,x_max,res)\n",
    "    y = np.linspace(y_min,y_max,res)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    pos = np.empty(X.shape + (2,))\n",
    "    pos[:, :, 0] = X; pos[:, :, 1] = Y\n",
    "  \n",
    "    \n",
    "    distances1, indices = knn1.kneighbors(np.transpose(np.array([X.flatten(),Y.flatten()])))\n",
    "    distances1 = np.mean(distances1,axis=1)\n",
    "    distances1 = distances1.reshape(X.shape[0],X.shape[1])\n",
    "    distances2, indices = knn2.kneighbors(np.transpose(np.array([X.flatten(),Y.flatten()])))\n",
    "    distances2 = np.mean(distances2,axis=1)\n",
    "    distances2 = distances2.reshape(X.shape[0],X.shape[1])\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,8))   \n",
    "    ax.scatter(Xdata1[:,0],Xdata1[:,1],c='g', label='Classe 1')\n",
    "    ax.scatter(Xdata2[:,0],Xdata2[:,1],c='b', label='Classe 2')\n",
    "    \n",
    "    Z=distances1-distances2\n",
    "    \n",
    "    ax.contourf(X, Y, Z, levels=[-10., 0, 10.], colors = ['lightsteelblue','mintcream'], alpha=.4)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    \n",
    "# Appel de la fonction  \n",
    "plotKNNdecision(pdf1,pdf2,X1,X2,x_min=0.5, y_min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Faire :\n",
    "\n",
    "Reprendre l'exercice avec le dataset2 et essayez d'obtenir les meilleurs résultats possibles sur ce dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
