{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9f008e",
   "metadata": {},
   "source": [
    "# Mécanisme d'attention\n",
    "Dans ce notebook, nous allons explorer un problème de type *sequence-to-sequence*. Pour cela nous allons créer un jeu de donnée *jouet* (toy dataset) et l'analyser avec un réseau convolutif 1D. Nous comparerons les performances avant et après l'introduction d'une couche d'**attention**, ce qui nous permettra de comprendre la pertinence de ce mécanisme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15332b87",
   "metadata": {},
   "source": [
    "### imports python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98bc2c",
   "metadata": {},
   "source": [
    "## 1. Génération des données\n",
    "On génère des sequences 1D contenant chacunes deux formes triangulaires et deux formes rectangulaires (step). La position et la hauteur de chacune des formes est choisie aléatoirement, la seule contrainte étant que les formes ne doivent pas se chevaucher.  \n",
    "**L'objectif du notebook sera d'entrainer un modèle permettant de transformer un séquence de telle manière que les deux triangles et les deux rectangles fassent la même hauteur, respectivement.**  \n",
    "[Source originale de l'exercice](https://fleuret.org/dlc/materials/dlc-handout-13-2-attention-mechanisms.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e672f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamètres pour la génération\n",
    "n_sequence = 1000 # nombres de séquences générées\n",
    "n_points = 100 # nombre de points par séquence\n",
    "width = 8 # largeur des triangles et step ajoutés dans les séquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_shape(x, y, center, height, width=8, triangle=False):\n",
    "    mask_up = (x <= center+width/2)\n",
    "    mask_down = (x >= center-width/2)\n",
    "    if triangle:\n",
    "        y[mask_down & (x<=center)] = height * (x[mask_down & (x<=center)]-center+width/2) / 4\n",
    "        y[mask_up & (x>center)] = height - height * (x[mask_up & (x>center)]-center) / 4\n",
    "    else:\n",
    "        y[mask_down & mask_up] = height\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ada01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_matrix(a):\n",
    "    x = np.reshape(a, (len(a), 1))\n",
    "    return x - x.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x_data = np.linspace(0, 100, 100)\n",
    "\n",
    "s = 0\n",
    "sequences_input = []\n",
    "sequences_target_shape = [] # pour les parties 2 & 3\n",
    "sequences_target_position = [] # pour la partie 4\n",
    "\n",
    "while s < n_sequence:\n",
    "    centers = np.sort(np.random.rand(4)*100) # génération aléatoire de la position des formes\n",
    "    if centers.min()-width/2 < 0 or centers.max()+width/2 > 100:\n",
    "        continue\n",
    "    \n",
    "    diff_matrix = np.abs(difference_matrix(centers))\n",
    "    diag_mask = np.eye(diff_matrix.shape[0], diff_matrix.shape[1], dtype=bool)\n",
    "    if diff_matrix[~diag_mask].min() >= width: # chevauchement ?\n",
    "        s += 1\n",
    "        heights = np.random.randint(2,29, 4) # génération aléatoire de la taille des formes\n",
    "        triangle_idxs = np.random.choice(4, 2, replace=False) # choix aléatoire de la position des triangles\n",
    "        mask_idx = np.zeros_like(heights).astype(bool)\n",
    "        mask_idx[triangle_idxs] = True\n",
    "\n",
    "        h_target_triangle = heights[mask_idx].mean() # cible = hauteur moyenne des triangles\n",
    "        h_target_step = heights[~mask_idx].mean() # cible = hauteur moyenne des rectangle\n",
    "        \n",
    "        h_target_left = heights[:2].mean() # cible = hauteur moyenne des 2 formes les plus à gauche\n",
    "        h_target_right = heights[2:].mean() # cible = hauteur moyenne des 2 formes les plus à droite\n",
    "\n",
    "        seq = np.zeros_like(x_data)\n",
    "        seq_target_shape = np.zeros_like(x_data)\n",
    "        seq_target_position = np.zeros_like(x_data)\n",
    "        for i, (c, h) in enumerate(zip(centers, heights)):\n",
    "            if i in triangle_idxs:\n",
    "                seq = make_shape(x_data, seq, c, h, triangle=True, width=width)\n",
    "                seq_target_shape = make_shape(x_data, seq_target_shape, c, h_target_triangle, triangle=True, width=width)\n",
    "                if i <2 :\n",
    "                    seq_target_position = make_shape(x_data, seq_target_position, c, h_target_left,\n",
    "                                                     triangle=True, width=width)\n",
    "                else:\n",
    "                    seq_target_position = make_shape(x_data, seq_target_position, c, h_target_right,\n",
    "                                                     triangle=True, width=width)\n",
    "            else:\n",
    "                seq = make_shape(x_data, seq, c, h)\n",
    "                seq_target_shape = make_shape(x_data, seq_target_shape, c, h_target_step)\n",
    "                if i <2 :\n",
    "                    seq_target_position = make_shape(x_data, seq_target_position, c, h_target_left, width=width)\n",
    "                else:\n",
    "                    seq_target_position = make_shape(x_data, seq_target_position, c, h_target_right, width=width)\n",
    "                \n",
    "        sequences_input.append(seq)\n",
    "        sequences_target_shape.append(seq_target_shape)\n",
    "        sequences_target_position.append(seq_target_position)\n",
    "        \n",
    "sequences_input = np.array(sequences_input).reshape(n_sequence, 1, 100)\n",
    "sequences_target_shape = np.array(sequences_target_shape).reshape(n_sequence, 1, 100)\n",
    "sequences_target_position = np.array(sequences_target_position).reshape(n_sequence, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec07811",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sequences_input[134,0], label='input')\n",
    "plt.plot(sequences_target_shape[134,0], label='target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f50b89",
   "metadata": {},
   "source": [
    "### Création des datasets pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8409fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_target, test_target = train_test_split(sequences_input,\n",
    "                                                                      sequences_target_shape,\n",
    "                                                                      test_size=0.25)\n",
    "\n",
    "# Normalisation des données\n",
    "mu, std = train_input.mean(), train_input.std()\n",
    "train_input = (train_input-mu)/std\n",
    "test_input = (test_input-mu)/std\n",
    "\n",
    "# Conversion des tableaux numpy vers des tenseurs reconnus par pytorch :\n",
    "train_input = torch.from_numpy(train_input).type(torch.FloatTensor)\n",
    "train_target = torch.from_numpy(train_target).type(torch.FloatTensor)\n",
    "train_data = TensorDataset(train_input, train_target)\n",
    "print('dataset entrainement : ', train_data)\n",
    "\n",
    "test_input = torch.from_numpy(test_input).type(torch.FloatTensor)\n",
    "test_target = torch.from_numpy(test_target).type(torch.FloatTensor)\n",
    "test_data = TensorDataset(test_input, test_target)\n",
    "print('dataset entrainement : ', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59315101",
   "metadata": {},
   "source": [
    "## 2. Modèle sans mécanisme d'attention\n",
    "**À faire** : Créer un modèle pytorch prenant en entrée une séquence et retournant en sortie une autre séquence, obtenue par l'application de 4 couches de convolution 1D, chacune composée de 64 convolutions de taille 5.  \n",
    "On choisira un padding permettant de conserver la longeur originale des séquences et on appliquera une fonction d'activation ReLU après chaque convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_conv1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_conv1d, self).__init__()\n",
    "        ## à compléter\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## à compléter\n",
    "\n",
    "        \n",
    "model_conv1d = Net_conv1d()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac906a4",
   "metadata": {},
   "source": [
    "### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres\n",
    "N_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size=50\n",
    "\n",
    "n_iteration_per_epoch = len(train_input) // batch_size +1\n",
    "n_test_loops = np.arange(0, N_epochs*n_iteration_per_epoch, n_iteration_per_epoch)\n",
    "\n",
    "# Outil de chargement des données\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0cc06",
   "metadata": {},
   "source": [
    "**À faire** : choisir une fonction de coût pertinente pour le problème considéré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function =  ## à compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7e27e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, loss_function, train_loader, test_loader, N_epochs):\n",
    "    losses = []   \n",
    "    val_losses = []\n",
    "    for epoch in range(N_epochs):  # Boucle sur les époques    \n",
    "        for inputs, targets in train_loader:\n",
    "            #Propagation en avant\n",
    "            preds = model(inputs) # Equivalent à model.forward(features)\n",
    "\n",
    "            #Calcul du coût\n",
    "            loss = loss_function(preds, targets)\n",
    "\n",
    "            #on sauvegarde la loss pour affichage futur\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            #Effacer les gradients précédents\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Calcul des gradients (rétro-propagation)\n",
    "            loss.backward()\n",
    "\n",
    "            #Mise à jour des poids : un pas de l'optimiseur\n",
    "            optimizer.step()\n",
    "        print('Epoque',epoch, 'loss', loss.item())\n",
    "        \n",
    "        val_losses.append(test_loop(model, test_loader))\n",
    "\n",
    "    print('Entrainement terminé')\n",
    "    \n",
    "    return losses, val_losses\n",
    "\n",
    "def test_loop(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        val_loss = []\n",
    "        for inputs, targets in test_loader:\n",
    "            #Propagation en avant\n",
    "            preds = model(inputs)\n",
    "\n",
    "            #Calcul du coût\n",
    "            loss = loss_function(preds, targets)\n",
    "\n",
    "            #on sauvegarde la loss pour affichage futur\n",
    "            val_loss.append(loss.item())\n",
    "        \n",
    "    return np.array(val_loss).mean()\n",
    "\n",
    "def draw_loss(losses, val_losses, n_test_loops):\n",
    "    # Afficher l'évolution de la fonction de coût\n",
    "    fig, axes = plt.subplots(figsize=(8,6))\n",
    "    axes.plot(losses,'r-',lw=2,label='Fonction de cout')\n",
    "    axes.plot(n_test_loops, val_losses,'b-',lw=2,label='Fonction de cout - validation')\n",
    "    axes.set_xlabel('N iterations',fontsize=18)\n",
    "    axes.set_ylabel('Cout',fontsize=18)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc='upper right',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b443630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_conv1d.parameters(), lr=learning_rate)\n",
    "losses, val_losses = train_loop(model_conv1d, optimizer, loss_function, train_dataloader, test_dataloader, N_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6961e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss(losses, val_losses, n_test_loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25341f6",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_conv1d = model_conv1d(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loss sur le dataset de test = {loss_function(test_output_conv1d, test_target):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 20\n",
    "\n",
    "#plt.plot(test_input[test_idx,0].numpy()*std+mu, label='input')\n",
    "plt.plot(test_output_conv1d[test_idx,0].detach().numpy(), label='prediction')\n",
    "plt.plot(test_target[test_idx,0].numpy(), label='target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f3cfd",
   "metadata": {},
   "source": [
    "## 3. Modèle avec couche d'attention\n",
    "\n",
    "**À faire** : modifier le modèle défini plus haut en remplaçant la couche de convolution centrale par une couche d'attention.  \n",
    "Rappel de la formule pour le mécanisme d'attention standard $$Y = \\text{softmax} \\left( QK^{T} \\right) V $$  \n",
    "On a les dimensions suivantes : $$ Q \\in \\mathcal{R}^{T\\times D}, K \\in \\mathcal{R}^{T'\\times D}, V \\in \\mathcal{R}^{T'\\times D'} $$\n",
    "où T (T') est la taille des séquences en entrée (sortie) de la couche d'attention, et D (D') est le nombre de *queries*/*keys* (*values*). Le nombre de *queries* et de *keys* doit correspondre, car les *queries* interrogent *keys*.  \n",
    "D' correspond aussi au nombre de canaux en sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, key_dim):\n",
    "        super().__init__()\n",
    "        self.conv_Q = nn.Conv1d(in_dim, out_dim, kernel_size=1, bias=False)\n",
    "        self.conv_K = nn.Conv1d(in_dim, key_dim, kernel_size=1, bias=False)\n",
    "        self.conv_V = nn.Conv1d(in_dim, key_dim, kernel_size=1, bias=False)\n",
    "        \n",
    "    def forward(self, x, return_attention=False):\n",
    "        ## à compléter\n",
    "    \n",
    "class Net_attention(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        ## à compléter\n",
    "        self.att = SelfAttentionLayer(64, 64, 64)\n",
    "        \n",
    "    def forward(self, x, return_attention=False):\n",
    "        ## à compléter\n",
    "        \n",
    "model_attention = Net_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2b33ac",
   "metadata": {},
   "source": [
    "### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a9477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_attention.parameters(), lr=learning_rate)\n",
    "\n",
    "losses, val_losses = train_loop(model_attention, optimizer, loss_function, train_dataloader, test_dataloader, N_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss(losses, val_losses, n_test_loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693a70d",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_att = model_attention(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac395ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(test_output_att, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(test_input[10,0].numpy()*std+mu, label='input')\n",
    "plt.plot(test_output_att[test_idx,0].detach().numpy(), label='prediction')\n",
    "plt.plot(test_target[test_idx,0].numpy(), label='target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0f361",
   "metadata": {},
   "source": [
    "### Visualisation de l'attention\n",
    "On souhaite représenter la matrice d'attention (taille 100x100) pour une séquence de test.  \n",
    "**À faire** : Modifier les classes `Net_attention` et `SelfAttentionLayer` afin qu'elles puissent retourner la matrice d'attention $$A = \\text{softmax} \\left( QK^{T} \\right) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, attention = model_attention(test_input[[test_idx]], return_attention=True)\n",
    "plt.pcolor(attention.squeeze().detach().numpy(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee65ff",
   "metadata": {},
   "source": [
    "## 4.Introduction au *positional encoding*\n",
    "La couche d'attention définie au-dessus ne prend condidère aucunement la position des formes. On va introduire un nouvel objectif qui va dépendre de la position des formes afin d'illuster ici l'intérêt, et la nécessité, d'utiliser un *positional encoding* pour certaines tâches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088570ee",
   "metadata": {},
   "source": [
    "### Nouvelle tâche\n",
    "Le nouvel objectif du réseau est de faire en sorte que les deux formes de gauche et les deux formes de droite aient la même hauteur, respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8940e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sequences_input[3,0], label='input')\n",
    "plt.plot(sequences_target_position[3,0], label='target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a32e07",
   "metadata": {},
   "source": [
    "On crée des nouveaux datasets et dataloaders avec les sequences cibles correspondant à la nouvelle tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5449ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_pos, test_input_pos, train_target_pos, test_target_pos = train_test_split(sequences_input,\n",
    "                                                                      sequences_target_position,\n",
    "                                                                      test_size=0.25)\n",
    "\n",
    "# Normalisation des données\n",
    "mu, std = train_input_pos.mean(), train_input_pos.std()\n",
    "train_input_pos = (train_input_pos-mu)/std\n",
    "test_input_pos = (test_input_pos-mu)/std\n",
    "\n",
    "# Conversion des tableaux numpy vers des tenseurs reconnus par pytorch :\n",
    "train_input_pos = torch.from_numpy(train_input_pos).type(torch.FloatTensor)\n",
    "train_target_pos = torch.from_numpy(train_target_pos).type(torch.FloatTensor)\n",
    "train_data_pos = TensorDataset(train_input_pos, train_target_pos)\n",
    "\n",
    "test_input_pos = torch.from_numpy(test_input_pos).type(torch.FloatTensor)\n",
    "test_target_pos = torch.from_numpy(test_target_pos).type(torch.FloatTensor)\n",
    "test_data_pos = TensorDataset(test_input_pos, test_target_pos)\n",
    "\n",
    "train_dataloader_pos = DataLoader(dataset=train_data_pos, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_pos = DataLoader(dataset=test_data_pos, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915fcaa",
   "metadata": {},
   "source": [
    "### Entrainement sans *positional encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4b793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_attention_noposition = Net_attention()\n",
    "\n",
    "optimizer = optim.Adam(model_attention_noposition.parameters(), lr=learning_rate)\n",
    "losses, val_losses = train_loop(model_attention_noposition, optimizer, loss_function, train_dataloader_pos, test_dataloader_pos, N_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f329d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss(losses, val_losses, n_test_loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505696f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_att_nopos = model_attention_noposition(test_input_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166bf74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(test_output_att_nopos, test_target_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 35\n",
    "\n",
    "#plt.plot(test_input[10,0].numpy()*std+mu, label='input')\n",
    "plt.plot(test_output_att_nopos[test_idx,0].detach().numpy(), label='prediction')\n",
    "plt.plot(test_target_pos[test_idx,0].numpy(), label='target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be632a6b",
   "metadata": {},
   "source": [
    "### Entrainement avec *positional encoding*\n",
    "On va encoder en binaire la position de chaque point des séquences, puis concaténer cet encodage avec nos inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_positional_encoder(len_seq):\n",
    "    c = np.ceil(np.log(len_seq) / np.log(2.0))\n",
    "    o = 2**torch.arange(c).unsqueeze(1)\n",
    "    pe = (torch.arange(len_seq).unsqueeze(0).div(o, rounding_mode = 'floor')) % 2\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a04b87",
   "metadata": {},
   "source": [
    "**À faire** : créer le *positional encoding* correspondant aux séquences dans notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e71c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = ## à compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab4f2c",
   "metadata": {},
   "source": [
    "**À faire** : modifier les données d'entrainement et de test en concaténant chaque séquence d'input avec le *positional encoding* (même encodage pour toutes les séquences).  \n",
    "Au final, pour un échantillon, on veux 8 canaux : 1 pour la séquence à proprement parler, et 7 additionels contenant le *positional encoding*.  \n",
    "Au niveau des dimensions , on cherche à voir un tenseur de taille (N, 8, 100) suite à la concaténation, plutôt que (N, 1, 100) avant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_pe = ## à compléter\n",
    "test_input_pe = ## à compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d17faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pos = TensorDataset(train_input_pe, train_target_pos)\n",
    "\n",
    "test_data_pos = TensorDataset(test_input_pe, test_target_pos)\n",
    "\n",
    "train_dataloader_pos = DataLoader(dataset=train_data_pos, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_pos = DataLoader(dataset=test_data_pos, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc2e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_attention_pe = Net_attention(in_channels=8)\n",
    "optimizer = optim.Adam(model_attention_pe.parameters(), lr=learning_rate)\n",
    "losses, val_losses = train_loop(model_attention_pe, optimizer, loss_function, train_dataloader_pos, test_dataloader_pos, N_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss(losses, val_losses, n_test_loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_att_pos = model_attention_pe(test_input_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2058ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(test_output_att_pos, test_target_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(test_input[10,0].numpy()*std+mu, label='input')\n",
    "plt.plot(test_output_att_pos[test_idx,0].detach().numpy(), label='prediction')\n",
    "plt.plot(test_target_pos[test_idx,0].numpy(), label='target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140885c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
