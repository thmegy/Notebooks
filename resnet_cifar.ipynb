{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d969362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/anaconda3/envs/openmmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62ee55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_uncertainty(method, tensor):\n",
    "    '''\n",
    "    Estimate classification uncertainty.\n",
    "    Arguments:\n",
    "    \n",
    "    method (str): name of method used to compute uncertainty\n",
    "    tensor (torch.tensor): output of classification network, dimension ( N(images), N(classes) )\n",
    "    Returns:\n",
    "    torch.tensor with dimension ( N(images) )\n",
    "    '''\n",
    "    tensor = torch.nn.functional.softmax(tensor, dim=1)\n",
    "    \n",
    "    if method == 'MarginSampling':\n",
    "        return margin_sampling(tensor)\n",
    "    elif method == 'Entropy':\n",
    "        return entropy(tensor)\n",
    "    elif method == 'VarRatio':\n",
    "        return var_ratio(tensor)\n",
    "\n",
    "def margin_sampling(tensor):\n",
    "    '''\n",
    "    Measure uncertainty as 1 - difference between probabilities of the two highest-ranking classes\n",
    "    '''\n",
    "    tensor_sorted = tensor.sort(dim=1, descending=True)[0] # sort class probabilities\n",
    "    return 1 - (tensor_sorted[:,0] - tensor_sorted[:,1])\n",
    "\n",
    "def entropy(tensor):\n",
    "    '''\n",
    "    Measure uncertainty as predictive entropy\n",
    "    '''\n",
    "    tensor_log = tensor.log()\n",
    "    return -(tensor*tensor_log).sum(dim=1)\n",
    "\n",
    "def var_ratio(tensor):\n",
    "    '''\n",
    "    Measure uncertainty as 1 - probability of highest-ranking class\n",
    "    '''\n",
    "    return 1 - tensor.max(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3fd78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_images(method, tensor, n_sel, **kwargs):\n",
    "    '''\n",
    "    Select set of unlabelled images to be added to training set\n",
    "    Arguments:\n",
    "    \n",
    "    method (str): name of method used to select images\n",
    "    tensor (torch.tensor): output of aggregate_uncertainty, dimension ( N(images) )\n",
    "    n_sel (int): number of images to select\n",
    "    Returns:\n",
    "    torch.tensor with index of selected images, dimension ( n_sel )\n",
    "    '''\n",
    "\n",
    "    if method == 'batch':\n",
    "        return batch_selection(tensor, n_sel, **kwargs)\n",
    "    elif method == 'maximum':\n",
    "        return maximum_selection(tensor, n_sel)\n",
    "    elif method == 'CoreSet':\n",
    "        return core_set(tensor, n_sel, **kwargs)\n",
    "\n",
    "def batch_selection(tensor, n_sel, **kwargs):\n",
    "    '''\n",
    "    Split randomly shuffled images in batches, compute aggregate score (sum) for each batch and select highest scores until n_sel is reached.\n",
    "    Necessary keyword argument:\n",
    "    batch_size (int): number of images per batch\n",
    "    '''\n",
    "    if (n_sel % kwargs['batch_size']) == 0:\n",
    "        batch_size_sel = int(n_sel / kwargs['batch_size'])\n",
    "    else:\n",
    "        batch_size_sel = (n_sel // kwargs['batch_size']) + 1\n",
    "    \n",
    "    r = torch.randperm(tensor.shape[0])\n",
    "    tensor_shuffle = tensor[r] # randomly shuffle images\n",
    "\n",
    "    batch_list = tensor_shuffle.split(kwargs['batch_size'])\n",
    "    batch_score_tensor = torch.tensor( [ b.sum().item() for b in batch_list ] )\n",
    "    batch_argmax = batch_score_tensor.sort(descending=True)[1][:batch_size_sel]\n",
    "\n",
    "    arg_sel = torch.concat( [r.split(kwargs['batch_size'])[ib] for ib in range(len(batch_list)) if ib in batch_argmax] )\n",
    "\n",
    "    return arg_sel\n",
    "\n",
    "def maximum_selection(tensor, n_sel):\n",
    "    '''\n",
    "    Select the n_sel images with the highest uncertainty.\n",
    "    '''\n",
    "    return tensor.sort(descending=True)[1][:n_sel]\n",
    "\n",
    "def core_set(tensor, n_sel, **kwargs):\n",
    "    '''\n",
    "    k-center greedy, limited to pool set and with distance weighted by uncertainty\n",
    "    '''\n",
    "    # compute matrix of distances between images\n",
    "    dist_mat = kwargs['embedding'] @ kwargs['embedding'].transpose(0, 1)\n",
    "    diag = dist_mat.diag().reshape( (kwargs['embedding'].shape[0], 1) )\n",
    "    dist_mat *= -2\n",
    "    dist_mat += diag\n",
    "    dist_mat += diag.transpose(0, 1)\n",
    "    dist_mat = torch.sqrt(dist_mat)\n",
    "\n",
    "    # choose first centroid randomly\n",
    "    centroids = torch.zeros(tensor.shape[0], dtype=torch.bool)\n",
    "    sel_idx = torch.randint( kwargs['embedding'].shape[0], (1,1) ).squeeze().item()\n",
    "    centroids[sel_idx] = True\n",
    "    \n",
    "    # select centroids (= data to be labelled) iteratively\n",
    "    for _ in range(n_sel-1):\n",
    "        d = dist_mat[~centroids][:,centroids] # remove centroids from row and keep only centroids for columns\n",
    "        unc = tensor[~centroids] # get uncertainty of images other than centroids\n",
    "        \n",
    "        mat_min = d.min(dim=1)[0] # get closest centroid for each image\n",
    "        weighted_mat_min = mat_min * unc # weight distance to closest centroid by uncertainty on image\n",
    "        sel_idx_ = weighted_mat_min.argmax().item() # select image with largest distance to closest centroid\n",
    "        sel_idx = torch.arange(tensor.shape[0])[~centroids][sel_idx_] # correct index for already selected images/rows\n",
    "        centroids[sel_idx] = True\n",
    "\n",
    "    return torch.arange(tensor.shape[0])[centroids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d519ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a95fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, embedding=False, loss_prediction=False):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        feat_map_1 = self.layer1(out)\n",
    "        feat_map_2 = self.layer2(feat_map_1)\n",
    "        feat_map_3 = self.layer3(feat_map_2)\n",
    "        out = self.avg_pool(feat_map_3)\n",
    "        feat_vec = out.view(out.size(0), -1)\n",
    "        out = self.fc(feat_vec)\n",
    "        if embedding:\n",
    "            return out, feat_vec # for core-set selection\n",
    "        elif loss_prediction:\n",
    "            return out, [feat_map_1, feat_map_2, feat_map_3]\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aafcb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossPredictionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossPredictionModule, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 64)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc_out = nn.Linear(3*64, 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        out1 = features[0].mean(dim=(-2,-1))\n",
    "        out1 = F.relu(self.fc1(out1))\n",
    "\n",
    "        out2 = features[1].mean(dim=(-2,-1))\n",
    "        out2 = F.relu(self.fc2(out2))\n",
    "\n",
    "        out3 = features[2].mean(dim=(-2,-1))\n",
    "        out3 = F.relu(self.fc3(out3))\n",
    "\n",
    "        out = torch.cat((out1, out2, out3), dim=1)\n",
    "        out = self.fc_out(out).squeeze()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "453576b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfcf512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(models, optimizers, train_loader, learning_rate, num_epochs, epoch_loss=48, margin=1, weight=1):\n",
    "    \n",
    "    models['resnet'].train()\n",
    "    if len(models) > 1:\n",
    "        models['module'].train()\n",
    "    total_step = len(train_loader)\n",
    "    curr_lr = learning_rate\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            if len(models) > 1:\n",
    "                criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "                \n",
    "                outputs, features = models['resnet'](images, loss_prediction=True)\n",
    "                loss_target = criterion(outputs, labels)\n",
    "                \n",
    "                if epoch > epoch_loss:\n",
    "                    # After 48 epochs, stop the gradient from the \n",
    "                    #loss prediction module propagated to the target model.\n",
    "                    features[0] = features[0].detach()\n",
    "                    features[1] = features[1].detach()\n",
    "                    features[2] = features[2].detach()\n",
    "                loss_pred = models['module'](features)\n",
    "\n",
    "                loss_pred_pairs = loss_pred.reshape(-1,2)\n",
    "                loss_target_pairs = loss_target.reshape(-1,2)\n",
    "                \n",
    "                ones = torch.ones(loss_target_pairs.shape[0], dtype=torch.double).to(device)\n",
    "                pm = torch.where(loss_target_pairs[:,0] > loss_target_pairs[:,1], ones, -1.)\n",
    "                \n",
    "                loss_module = -1 * pm * (loss_pred_pairs[:,0] - loss_pred_pairs[:,1]) + margin\n",
    "                loss_module = torch.where(loss_module > 0, loss_module, 0.)\n",
    "\n",
    "                loss = loss_target.mean() + weight * loss_module.mean()\n",
    "\n",
    "#                crit_module = MarginRankingLoss_learning_loss()\n",
    "#                loss_module = crit_module(loss_pred, loss_target)                \n",
    "#                loss = loss_target.mean() + weight * loss_module\n",
    "                \n",
    "#                loss_module = LossPredLoss(loss_pred, loss_target)\n",
    "#                loss = loss_target.mean() + weight * 2 * loss_module\n",
    "\n",
    "            else:\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                outputs = models['resnet'](images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizers['resnet'].step()\n",
    "            if len(models) > 1:\n",
    "                optimizers['module'].step()\n",
    "\n",
    "            if epoch % 40 == 0:\n",
    "                if i == 10:\n",
    "                    print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        # Decay learning rate\n",
    "        if (epoch+1) % 20 == 0:\n",
    "            curr_lr /= 3\n",
    "            update_lr(optimizer, curr_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "895804b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test_model(models, test_loader):\n",
    "    models['resnet'].eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = models['resnet'](images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "        return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e8b076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd90997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cec6cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c15379ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001\n",
    "epoch_loss = 48\n",
    "\n",
    "n_round = 10\n",
    "n_sel = 1000\n",
    "n_repeat = 3\n",
    "unc_method = 'MarginSampling'\n",
    "sel_method = 'maximum'\n",
    "\n",
    "loss_prediction = False\n",
    "if unc_method == 'LossPrediction':\n",
    "    loss_prediction = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2511a9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "Accuracy of the model on the test images: 49.92 %\n",
      "\n",
      "Round 1\n",
      "Epoch [1/80], Step [11/20] Loss: 1.7958\n",
      "Epoch [41/80], Step [11/20] Loss: 0.3983\n",
      "Accuracy of the model on the test images: 57.55 %\n",
      "\n",
      "Round 2\n",
      "Epoch [1/80], Step [11/30] Loss: 1.1775\n",
      "Epoch [41/80], Step [11/30] Loss: 0.2481\n",
      "Accuracy of the model on the test images: 63.52 %\n",
      "\n",
      "Round 3\n",
      "Epoch [1/80], Step [11/40] Loss: 1.1879\n",
      "Epoch [41/80], Step [11/40] Loss: 0.1324\n",
      "Accuracy of the model on the test images: 68.0 %\n",
      "\n",
      "Round 4\n",
      "Epoch [1/80], Step [11/50] Loss: 1.0221\n",
      "Epoch [41/80], Step [11/50] Loss: 0.0902\n",
      "Accuracy of the model on the test images: 71.75 %\n",
      "\n",
      "Round 5\n",
      "Epoch [1/80], Step [11/60] Loss: 0.8155\n",
      "Epoch [41/80], Step [11/60] Loss: 0.1016\n",
      "Accuracy of the model on the test images: 74.54 %\n",
      "\n",
      "Round 6\n",
      "Epoch [1/80], Step [11/70] Loss: 0.7596\n",
      "Epoch [41/80], Step [11/70] Loss: 0.0654\n",
      "Accuracy of the model on the test images: 76.17 %\n",
      "\n",
      "Round 7\n",
      "Epoch [1/80], Step [11/80] Loss: 0.6018\n",
      "Epoch [41/80], Step [11/80] Loss: 0.1415\n",
      "Accuracy of the model on the test images: 78.13 %\n",
      "\n",
      "Round 8\n",
      "Epoch [1/80], Step [11/90] Loss: 0.5002\n",
      "Epoch [41/80], Step [11/90] Loss: 0.0351\n",
      "Accuracy of the model on the test images: 79.01 %\n",
      "\n",
      "Round 9\n",
      "Epoch [1/80], Step [11/100] Loss: 0.4404\n",
      "Epoch [41/80], Step [11/100] Loss: 0.0675\n",
      "Accuracy of the model on the test images: 79.92 %\n",
      "\n",
      "Round 0\n",
      "Accuracy of the model on the test images: 53.06 %\n",
      "\n",
      "Round 1\n",
      "Epoch [1/80], Step [11/20] Loss: 1.5435\n",
      "Epoch [41/80], Step [11/20] Loss: 0.4023\n",
      "Accuracy of the model on the test images: 61.65 %\n",
      "\n",
      "Round 2\n",
      "Epoch [1/80], Step [11/30] Loss: 1.6573\n",
      "Epoch [41/80], Step [11/30] Loss: 0.1711\n",
      "Accuracy of the model on the test images: 66.38 %\n",
      "\n",
      "Round 3\n",
      "Epoch [1/80], Step [11/40] Loss: 1.0718\n",
      "Epoch [41/80], Step [11/40] Loss: 0.1657\n",
      "Accuracy of the model on the test images: 70.02 %\n",
      "\n",
      "Round 4\n",
      "Epoch [1/80], Step [11/50] Loss: 1.1669\n",
      "Epoch [41/80], Step [11/50] Loss: 0.1353\n",
      "Accuracy of the model on the test images: 73.46 %\n",
      "\n",
      "Round 5\n",
      "Epoch [1/80], Step [11/60] Loss: 0.7242\n",
      "Epoch [41/80], Step [11/60] Loss: 0.0735\n",
      "Accuracy of the model on the test images: 75.07 %\n",
      "\n",
      "Round 6\n",
      "Epoch [1/80], Step [11/70] Loss: 1.0082\n",
      "Epoch [41/80], Step [11/70] Loss: 0.1098\n",
      "Accuracy of the model on the test images: 77.09 %\n",
      "\n",
      "Round 7\n",
      "Epoch [1/80], Step [11/80] Loss: 0.8459\n",
      "Epoch [41/80], Step [11/80] Loss: 0.1225\n",
      "Accuracy of the model on the test images: 78.52 %\n",
      "\n",
      "Round 8\n",
      "Epoch [1/80], Step [11/90] Loss: 0.6705\n",
      "Epoch [41/80], Step [11/90] Loss: 0.1159\n",
      "Accuracy of the model on the test images: 80.05 %\n",
      "\n",
      "Round 9\n",
      "Epoch [1/80], Step [11/100] Loss: 0.3264\n",
      "Epoch [41/80], Step [11/100] Loss: 0.0743\n",
      "Accuracy of the model on the test images: 81.06 %\n",
      "\n",
      "Round 0\n",
      "Accuracy of the model on the test images: 48.27 %\n",
      "\n",
      "Round 1\n",
      "Epoch [1/80], Step [11/20] Loss: 1.5467\n",
      "Epoch [41/80], Step [11/20] Loss: 0.2553\n",
      "Accuracy of the model on the test images: 58.75 %\n",
      "\n",
      "Round 2\n",
      "Epoch [1/80], Step [11/30] Loss: 1.6374\n",
      "Epoch [41/80], Step [11/30] Loss: 0.2271\n",
      "Accuracy of the model on the test images: 64.55 %\n",
      "\n",
      "Round 3\n",
      "Epoch [1/80], Step [11/40] Loss: 1.7659\n",
      "Epoch [41/80], Step [11/40] Loss: 0.1300\n",
      "Accuracy of the model on the test images: 67.89 %\n",
      "\n",
      "Round 4\n",
      "Epoch [1/80], Step [11/50] Loss: 0.8604\n",
      "Epoch [41/80], Step [11/50] Loss: 0.1338\n",
      "Accuracy of the model on the test images: 71.47 %\n",
      "\n",
      "Round 5\n",
      "Epoch [1/80], Step [11/60] Loss: 1.0641\n",
      "Epoch [41/80], Step [11/60] Loss: 0.0858\n",
      "Accuracy of the model on the test images: 73.54 %\n",
      "\n",
      "Round 6\n",
      "Epoch [1/80], Step [11/70] Loss: 0.6646\n",
      "Epoch [41/80], Step [11/70] Loss: 0.0593\n",
      "Accuracy of the model on the test images: 75.53 %\n",
      "\n",
      "Round 7\n",
      "Epoch [1/80], Step [11/80] Loss: 0.5060\n",
      "Epoch [41/80], Step [11/80] Loss: 0.0749\n",
      "Accuracy of the model on the test images: 76.84 %\n",
      "\n",
      "Round 8\n",
      "Epoch [1/80], Step [11/90] Loss: 0.5263\n",
      "Epoch [41/80], Step [11/90] Loss: 0.0444\n",
      "Accuracy of the model on the test images: 78.61 %\n",
      "\n",
      "Round 9\n",
      "Epoch [1/80], Step [11/100] Loss: 0.5321\n",
      "Epoch [41/80], Step [11/100] Loss: 0.1514\n",
      "Accuracy of the model on the test images: 79.43 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "for irepeat in range(n_repeat):\n",
    "\n",
    "    train, pool = torch.utils.data.random_split(train_dataset, [1000, len(train_dataset)-1000])\n",
    "    #pool, _ = torch.utils.data.random_split(pool, [int(len(pool)/2), int(len(pool)/2)]) # ugly hack to reduce memory for Core-set\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train,\n",
    "                                               batch_size=100, \n",
    "                                               shuffle=True)\n",
    "    accuracy_list = []\n",
    "\n",
    "    # create model\n",
    "    models = {}\n",
    "    model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
    "    models['resnet'] = model\n",
    "    if loss_prediction:\n",
    "        module = LossPredictionModule().to(device)\n",
    "        models['module'] = module\n",
    "\n",
    "    for ir in range(n_round):\n",
    "        print(f'Round {ir}')\n",
    "        \n",
    "        optimizers = {}        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        optimizers['resnet'] = optimizer\n",
    "        if loss_prediction:\n",
    "            optim_module = torch.optim.Adam(module.parameters(), lr=learning_rate)\n",
    "            optimizers['module'] = optim_module\n",
    "\n",
    "        # train\n",
    "        train_model(models, optimizers, train_loader, learning_rate, num_epochs)\n",
    "\n",
    "        # test\n",
    "        accuracy_list.append( test_model(models, test_loader) )\n",
    "\n",
    "        if sel_method == 'random':\n",
    "            sel_idx = torch.randint(len(pool), (n_sel,))\n",
    "        else:\n",
    "            # inference on pool set\n",
    "            # select from subset of 10000 images, to avoid bias due to similar images with high uncertainty\n",
    "            pool_subset, pool_rest = torch.utils.data.random_split(pool, [10000, len(pool)-10000])\n",
    "            pool_subset_loader = torch.utils.data.DataLoader(dataset=pool_subset,\n",
    "                                                              batch_size=100, \n",
    "                                                              shuffle=False)\n",
    "\n",
    "            uncertainty = []\n",
    "            embedding = []\n",
    "\n",
    "            models['resnet'].eval()\n",
    "            if loss_prediction:\n",
    "                models['module'].eval()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                for images, labels in pool_subset_loader:\n",
    "                    images = images.to(device)\n",
    "                    if loss_prediction:\n",
    "                        features = models['resnet'](images, loss_prediction=True)[1]\n",
    "                        unc = models['module'](features)                        \n",
    "                    else:\n",
    "                        if sel_method == 'CoreSet':\n",
    "                            outputs, embed = models['resnet'](images, embedding=True)\n",
    "                            embedding.append(embed)\n",
    "                        else:\n",
    "                            outputs = models['resnet'](images)\n",
    "                        unc = estimate_uncertainty(unc_method, outputs)\n",
    "                    uncertainty.append(unc)\n",
    "            uncertainty = torch.cat(uncertainty)\n",
    "            \n",
    "            if sel_method == 'CoreSet':\n",
    "                embedding = torch.cat(embedding)\n",
    "                lin_mat = torch.ones([embedding.shape[1], 4], device=f'cuda:{torch.cuda.current_device()}')\n",
    "                embedding = embedding @ lin_mat\n",
    "                del lin_mat\n",
    "\n",
    "            # select images to label from pool and add to training set\n",
    "            sel_idx = select_images(sel_method, uncertainty, n_sel, batch_size=10, embedding=embedding)\n",
    "        selected = torch.utils.data.Subset(pool_subset, sel_idx)\n",
    "        train = torch.utils.data.ConcatDataset([train, selected])\n",
    "\n",
    "        # keep only non-selected images in pool\n",
    "        mask = torch.ones(len(pool_subset), dtype=torch.bool)\n",
    "        mask[sel_idx] = False\n",
    "        not_sel_idx = torch.arange(mask.shape[0])[mask]\n",
    "        pool = torch.utils.data.ConcatDataset( [pool_rest, torch.utils.data.Subset(pool_subset, not_sel_idx)] )\n",
    "\n",
    "        # make new loaders\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train,\n",
    "                                                   batch_size=100, \n",
    "                                                   shuffle=True)\n",
    "        pool_loader = torch.utils.data.DataLoader(dataset=pool,\n",
    "                                                   batch_size=100, \n",
    "                                                   shuffle=False)\n",
    "        torch.cuda.empty_cache()\n",
    "        print('')\n",
    "\n",
    "    if sel_method == 'random':\n",
    "        with open(f'output/random_{irepeat}.json', 'w') as f:\n",
    "            json.dump(accuracy_list, f)\n",
    "    else:\n",
    "        with open(f'output/{unc_method}_{sel_method}_{irepeat}.json', 'w') as f:\n",
    "            json.dump(accuracy_list, f)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3ac59ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f74c0e7fa50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.permute(selected[0][0].cpu(), (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9627ff7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([100])\n",
      "<class 'torch.Tensor'> torch.Size([100, 3, 32, 32])\n",
      "<class 'torch.Tensor'> torch.Size([100, 64, 1, 1])\n",
      "<class 'torch.Tensor'> torch.Size([100, 64])\n",
      "<class 'torch.Tensor'> torch.Size([100, 10])\n",
      "<class 'torch.Tensor'> torch.Size([100])\n",
      "<class 'torch.Tensor'> torch.Size([49000])\n",
      "<class 'torch.Tensor'> torch.Size([49000, 4])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16, 3, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([10, 64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([10])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([16, 3, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([10, 64])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([49000])\n",
      "<class 'torch.Tensor'> torch.Size([49000, 16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16, 3, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([10])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([10, 64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16, 3, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16, 3, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([10, 64])\n",
      "<class 'torch.Tensor'> torch.Size([10, 64])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([16, 3, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 16, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([32])\n",
      "<class 'torch.Tensor'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([10, 64])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 32, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 64, 3, 3])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n",
      "<class 'torch.Tensor'> torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4385525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "input = torch.randn(4, 5, requires_grad=True)\n",
    "target = torch.empty(4, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2247ed7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7518, -0.5871,  0.8098,  0.0913, -0.3258],\n",
       "        [ 1.5572, -0.1011,  0.2457,  1.6983, -0.3345],\n",
       "        [ 0.2467,  0.2393, -0.4506, -0.3468, -0.1449],\n",
       "        [ 0.6332, -0.3069, -0.3725, -0.7686, -0.7112]], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19077fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 1\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "loss_target = criterion(input, target)\n",
    "loss_pred = torch.randn(loss_target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba68cff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4954, 1.0160, 1.3208, 2.2186], grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62e7a38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7524, -0.1346,  0.0573, -1.2509])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c754318c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7524, -0.1346],\n",
      "        [ 0.0573, -1.2509]])\n",
      "tensor([[2.4954, 1.0160],\n",
      "        [1.3208, 2.2186]], grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor([ 1., -1.], dtype=torch.float64)\n",
      "tensor([1.6178, 2.3082], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.6178, 2.3082], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_pred_pairs = loss_pred.reshape(-1,2)\n",
    "print(loss_pred_pairs)\n",
    "loss_target_pairs = loss_target.reshape(-1,2)\n",
    "print(loss_target_pairs)\n",
    "ones = torch.ones(loss_target_pairs.shape[0], dtype=torch.double)\n",
    "pm = torch.where(loss_target_pairs[:,0] > loss_target_pairs[:,1], ones, -1.)\n",
    "print(pm)\n",
    "\n",
    "loss_module = -1 * pm * (loss_pred_pairs[:,0] - loss_pred_pairs[:,1]) + margin\n",
    "print(loss_module)\n",
    "loss_module = torch.where(loss_module > 0, loss_module, 0.)\n",
    "loss_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b16f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n",
    "    assert len(input) % 2 == 0, 'the batch size is not even.'\n",
    "    assert input.shape == input.flip(0).shape\n",
    "    \n",
    "    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n",
    "    target = (target - target.flip(0))[:len(target)//2]\n",
    "    target = target.detach()\n",
    "    one = 2 * torch.sign(torch.clamp(target, min=0)) - 1 # 1 operation which is defined by the authors\n",
    "    if reduction == 'mean':\n",
    "        loss = torch.sum(torch.clamp(margin - one * input, min=0))\n",
    "        loss = loss / input.size(0) # Note that the size of input is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = torch.clamp(margin - one * input, min=0)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8f79082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6548)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LossPredLoss(loss_pred, loss_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a07a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarginRankingLoss_learning_loss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(MarginRankingLoss_learning_loss, self).__init__()\n",
    "        self.margin = margin\n",
    "    def forward(self, inputs, targets):\n",
    "        random = torch.randperm(inputs.size(0))\n",
    "        pred_loss = inputs[random]\n",
    "        pred_lossi = inputs[:inputs.size(0)//2]\n",
    "        pred_lossj = inputs[inputs.size(0)//2:]\n",
    "        target_loss = targets.reshape(inputs.size(0), 1)\n",
    "        target_loss = target_loss[random]\n",
    "        target_lossi = target_loss[:inputs.size(0)//2]\n",
    "        target_lossj = target_loss[inputs.size(0)//2:]\n",
    "        final_target = torch.sign(target_lossi - target_lossj)\n",
    "        \n",
    "        return F.margin_ranking_loss(pred_lossi, pred_lossj, final_target, margin=self.margin, reduction='mean')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "915d7a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0291, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = MarginRankingLoss_learning_loss()\n",
    "criterion(loss_pred, loss_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
